{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем список обменников\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"https://www.cointelligence.com/exchanges_list/\") \n",
    "\n",
    "page = BeautifulSoup(r.text)\n",
    "\n",
    "exchange_list = []\n",
    "#Выбираем все названия бирж\n",
    "data = page.find_all(\"td\", {\"class\": \"name\"})\n",
    "for i in data:\n",
    "    exchange_list.append(i.contents[2].text.replace(\"[SCAM Alert]\",\"\")) # Убирам скам алерт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем новости\n",
    "from rss_parser import Parser\n",
    "import datetime\n",
    "#Топики\n",
    "TOPICS = [\"hacker attack\",\"law enforcement\",\"fraud\",\"funds withdrawal\"]\n",
    "#Ивенты которые получим в виде словаря {ecxchange - обменник, topic - топик из списка топриков, date - дата события}\n",
    "events = []\n",
    "\n",
    "for exchange in exchange_list:\n",
    "    now = datetime.datetime.now()\n",
    "    five_days_ago = now - datetime.timedelta(days=5)\n",
    "    for topic in TOPICS:\n",
    "        url = f\"http://news.google.com/news?q={exchange}%20{topic}&as_qdr=w&output=rss\"\n",
    "        r = requests.get(url)\n",
    "        parser = Parser(xml=r.content)\n",
    "        feed = parser.parse()\n",
    "        for item in feed.feed:\n",
    "            # У гугл новостей нет фильтра по датам для RSS. Фильтруем\n",
    "            event_date = datetime.datetime.strptime(item.publish_date,\"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "            if event_date > five_days_ago:\n",
    "                events.append({\"exchange\":exchange, \"topic\":topic, \"date\":item.publish_date})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стоп слова - отсеиваем сообщения ботов, выдающих статистику\n",
    "\n",
    "STOP_WORDS = [\"alert\",\"24h volume\",\"previous alt\", \"rewards\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 Too Many Requests\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "import datetime\n",
    "\n",
    "# Стоп слова - отсеиваем сообщения ботов, выдающих статистику\n",
    "\n",
    "STOP_WORDS = [\"alert\",\"24h volume\",\"previous alt\", \"rewards\"]\n",
    "\n",
    "def stop_words_check(text):\n",
    "    for word in STOP_WORDS:\n",
    "        if text.find(word)!=-1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "barier = \"AAAAAAAAAAAAAAAAAAAAAEagDAEAAAAAV%2Bw9NxUw3WDjUg%2FO4ad3WdOjRYc%3DL5tcIoSkdJGUVEnGxmPGTzqTD7LrHraSWS5crTnZHHZ74OWB3d\"\n",
    "\n",
    "client = tweepy.Client(bearer_token=barier)\n",
    "\n",
    "# Удаяем лики и спец символы\n",
    "def clean_tweet(tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "result = []\n",
    "authors = [] #Список авторов чтоб избежать повторений\n",
    "texts =[]\n",
    "\n",
    "\n",
    "\n",
    "for event in events:\n",
    "    if len(result)>250:\n",
    "        break #Прерываем если достигли нужного количества результатов\n",
    "    event_date = datetime.datetime.strptime(event[\"date\"],\"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "    start_date = event_date - datetime.timedelta(hours=5)\n",
    "    end_date = event_date+datetime.timedelta(hours=5)\n",
    "    #ДатаВремя до должно быть на 10 сек меньше даты и времени запроса (на всякий лучай берем 20 сек)\n",
    "    if end_date > datetime.datetime.utcnow() - datetime.timedelta(seconds=20):\n",
    "        end_date = datetime.datetime.utcnow() - datetime.timedelta(seconds=20) \n",
    "  \n",
    "    try:\n",
    "        tweets = client.search_recent_tweets(query=event[\"exchange\"], \n",
    "                                        end_time=end_date, \n",
    "                                        start_time=start_date,\n",
    "                                        tweet_fields = [\"author_id\",\"text\",\"created_at\"]\n",
    "                                        )\n",
    "        if tweets.data:\n",
    "            for tweet in tweets.data:\n",
    "                #Проверка на стоп слова\n",
    "                if stop_words_check(tweet.text.lower()):\n",
    "                    continue\n",
    "                # Фильтруем чтобы были уникальные автора\n",
    "                if tweet.author_id in authors:\n",
    "                    continue\n",
    "\n",
    "                if tweet.text in texts:\n",
    "                    continue\n",
    "\n",
    "                analysis = TextBlob(clean_tweet(tweet.text)) # Сентиментальный анализ\n",
    "                if (analysis.sentiment.polarity < 0): #Если новость негативная\n",
    "                    result.append({\"author\":tweet.author_id,\n",
    "                                   \"date\":tweet.created_at,\n",
    "                                   \"exchange\":event[\"exchange\"],\n",
    "                                   \"text\":tweet.text,\n",
    "\n",
    "                                   })\n",
    "                    authors.append(tweet.author_id)\n",
    "                    texts.append(tweet.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(60*15) #Ждем 15 минут если уперлись в лимит               \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Находим имена юзеров\n",
    "for i in result:\n",
    "    user = client.get_user(id=int(i[\"author\"]))\n",
    "    try:\n",
    "        i[\"author_name\"] = user.data.username\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняем в csv\n",
    "with open(\"res.csv\",\"w\") as f:\n",
    "    for i in result:\n",
    "        text =i['text'].replace('\\n','')\n",
    "        f.write(f\"{i['author_name']};{i['date']};{i['exchange']};{text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe57c59b4ddc82e60717f221751dc9bede108914aaea2a69dd1e32c0f57dddb3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
